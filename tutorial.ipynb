{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Project Setup: Colab Setup\n",
        "==========================\n",
        "\n",
        "To set up Google Colab for the research project, follow these steps to ensure your progress is saved. These\n",
        "instructions are also detailed in the accompanying document, which includes images for guidance.\n",
        "\n",
        "First, connect Google Colab to your Google account and Google Drive. This setup ensures your work is autosaved and\n",
        "preserved even if you close your browser tab or window. Next, open the tutorial notebook in Colab, click on\n",
        "the \"File\" tab in the top-left corner, and select \"Save a copy in Drive.\" This action will create a duplicate of the\n",
        "notebook and open it in a new tab titled \"Copy of tutorial.ipynb.\"\n",
        "\n",
        "In the new tab, rename the notebook by clicking on its title in the top-left corner. Change it\n",
        "from \"Copy of tutorial.ipynb\" to \"YOUR_NAME_tutorial.ipynb,\" replacing \"YOUR_NAME\" with your actual name.\n",
        "With these steps complete, the notebook will now autosave your changes to your Google Drive, ensuring your\n",
        "progress is retained.\n",
        "\n",
        "Project Setup: Software Installation\n",
        "====================================\n",
        "\n",
        "Next, install the Python software libraries required for this research project. In Google Colab, this can be done\n",
        "easily by running the cell below in the Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install autoconf==2024.11.13.2 autofit==2024.11.13.2 autoarray==2024.11.13.2 autogalaxy==2024.11.13.2 pyvis==0.3.2 dill==0.3.1.1 dynesty==2.1.4 emcee==3.1.6 nautilus-sampler==1.0.4 timeout_decorator==0.5.0 anesthetic==2.8.14 --no-deps\n",
        "!pip install numba"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Project Setup: Repository Clone\n",
        "===============================\n",
        "\n",
        "The code below downloads the project files from the GitHub repository and stores them in your Google Colab\n",
        "directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!git clone https://github.com/Jammy2211/BSc_Galaxies_Project"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Project Setup: Working Directory\n",
        "================================\n",
        "\n",
        "On the left hand side of your Google Collab window, you will see a file explorer. Click on the folder icon. This will\n",
        "open the file explorer. \n",
        "\n",
        "The screenshot below shows what should be displayed, for now you do not need to worry about the contents\n",
        "of this folder but later you will use it to inspect the output of the code you run:\n",
        "\n",
        "![ColabGolder](https://github.com/Jammy2211/BSc_Galaxies_Project/blob/master/Colab_Example_Folder.png?raw=true)\n",
        "\n",
        "The `content` folder is the root directory of your Google Colab environment, within which is a folder \n",
        "named `BSc_Galaxies_Project`. This folder contains all the files and scripts for the project, which were downloaded \n",
        "by the repository clone command above.\n",
        "\n",
        "The Python working directory defines where Python looks for data files and scripts to load. To ensure the working\n",
        "directory is correctly set to the `BSc_Galaxies_Project` folder, run the cell below. This cell also updates\n",
        "configuration file paths to ensure they point to the correct directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from autoconf import conf\n",
        "\n",
        "os.chdir(\"/content/BSc_Galaxies_Project\")\n",
        "\n",
        "conf.instance.push(\n",
        "    new_path=\"/content/BSc_Galaxies_Project/config\",\n",
        "    output_path=\"/content/BSc_Galaxies_Project/output\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BSc Galaxies Project: Introduction\n",
        "==================================\n",
        "\n",
        "Nearly a century ago, Edwin Hubble famously classified galaxies into three distinct groups: ellipticals, spirals and\n",
        "irregulars. He produced a diagram of these galaxies, called the Hubble Tuning Fork, which is shown below and still\n",
        "used by astronomers in the modern day:\n",
        "\n",
        "![HubbleTuning](https://github.com/Jammy2211/autogalaxy_workspace/blob/main/scripts/howtogalaxy/chapter_1_introduction/HubbleTuningFork.jpg?raw=true)\n",
        "\n",
        "To make his diagram, Hubble looked at images of each galaxy in his sample, and subjectively judged by eye how to\n",
        "classify them. Today, Astronomers use computer software, statistical algorithms and image processing techniques to\n",
        "perform this task in a more quantifiable and objective way.\n",
        "\n",
        "These tutorials will teach you how to perform this analysis yourself, using the open-source software\n",
        "package **PyAutoGalaxy**. By the end, you\u2019ll be able to take an image of a galaxy and study its morphology and\n",
        "structure using the same techniques that professional astronomers use today.\n",
        "\n",
        "Tutorial 1: Grids And Galaxies\n",
        "==============================\n",
        "\n",
        "In this tutorial, we will introduce the fundamental concepts and quantities used to study galaxy morphology.\n",
        "These concepts will enable us to create images of galaxies and analyze how their light is distributed across space.\n",
        "Additionally, we will explore how adjusting various properties of galaxies can alter their appearance. For instance,\n",
        "we can change the size of a galaxy, rotate it, or modify its brightness.\n",
        "\n",
        "To create these images, we first need to define 2D grids of $(y, x)$ coordinates. We will shift and rotate these\n",
        "grids to manipulate the appearance of the galaxy in the generated images. The grid will serve as the input for light\n",
        "profiles, which are analytic functions that describe the distribution of a galaxy's light. By evaluating these light\n",
        "profiles on the grid, we can effectively generate images that represent the structure and characteristics of galaxies.\n",
        "\n",
        "Here is an overview of what we'll cover in this tutorial:\n",
        "\n",
        "- **Grids**: Create a uniform grid of $(y,x)$ coordinates and show how it can be used to measure the light of a galaxy.\n",
        "- **Geometry**: How to shift and rotate a grid, and convert it to elliptical coordinates.\n",
        "- **Light Profiles**: Using light profiles, analytic functions that describe how a galaxy's light is distributed.\n",
        "- **Galaxies**: Creating galaxies containing light profiles and computing the image of a galaxy.\n",
        "- **Units**: Converting the units of a galaxy's image to physical units like kiloparsecs.\n",
        "\n",
        "The imports below are required to run the tutorials in a Jupyter notebook. They also import the\n",
        "`autogalaxy` package and the `autogalaxy.plot` module which are used throughout the tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Grids__\n",
        "\n",
        "A `Grid2D` is a set of two-dimensional $(y,x)$ coordinates that represent points in space where we evaluate the \n",
        "light emitted by a galaxy.\n",
        "\n",
        "Each coordinate on the grid is referred to as a 'pixel'. This is because we use the grid to create the image of a\n",
        "galaxy at each of these coordinates, meaning that each coordinate maps to the centre of each pixel in this image.\n",
        "\n",
        "Grids are defined in units of 'arc-seconds' (\"). An arc-second is a unit of angular measurement used by astronomers to \n",
        "describe the apparent size of objects in the sky.\n",
        "\n",
        "The `pixel_scales` parameter sets how many arc-seconds each pixel represents. For example, if `pixel_scales=0.1`, \n",
        "then each pixel covers 0.1\" of the sky.\n",
        "\n",
        "Below, we create a uniform 2D grid of 101 x 101 pixels with a pixel scale of 0.1\", corresponding to an area \n",
        "of 10.1\" x 10.1\", spanning from -5.05\" to 5.05\" in both the y and x directions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid = ag.Grid2D.uniform(\n",
        "    shape_native=(\n",
        "        101,\n",
        "        101,\n",
        "    ),  # The dimensions of the grid, which here is 101 x 101 pixels.\n",
        "    pixel_scales=0.1,  # The conversion factor between pixel units and arc-seconds.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize this grid as a uniform grid of dots, each representing a coordinate where the light is measured."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_plotter = aplt.Grid2DPlotter(grid=grid)\n",
        "grid_plotter.set_title(\"Uniform Grid of Coordinates\")\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each coordinate in the grid corresponds to an arc-second position. Below, we print a few of these coordinates to see \n",
        "the values.\n",
        "\n",
        "To print these values, we use the `native` attribute of the grid, which returns the grid as a 2D NumPy array of\n",
        "shape [total_y_pixels, total_x_pixels, 2].\n",
        "\n",
        "The `native` attribute is used to access many properties of numpy arrays through this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"(y,x) pixel 0:\")\n",
        "print(grid.native[0, 0])  # The coordinate of the first pixel.\n",
        "print(\"(y,x) pixel 1:\")\n",
        "print(grid.native[0, 1])  # The coordinate of the second pixel.\n",
        "print(\"(y,x) pixel 2:\")\n",
        "print(grid.native[0, 2])  # The coordinate of the third pixel.\n",
        "print(\"(y,x) pixel 100:\")\n",
        "print(grid.native[1, 0])  # The coordinate of the 100th pixel.\n",
        "print(\"...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also check the `shape_native` of the `Grid2D` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(grid.native.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Exercise: Try creating grids with different `shape_native` and `pixel_scales` using the `ag.Grid2D.uniform()` function above.  Print the grid (y,x) coordinates and observe how they change when you adjust `shape_native` and `pixel_scales`.*\n",
        "\n",
        "__Geometry__\n",
        "\n",
        "The above grid is centered on the origin $(0.0\", 0.0\")$. Sometimes, we need to shift the grid to be centered on a \n",
        "specific point, like the center of a galaxy.\n",
        "\n",
        "We can shift the grid to a new center, $(y_c, x_c)$, by subtracting this center from each coordinate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre = (0.3, 0.5)  # Shifting the grid to be centered at y=1.0\", x=2.0\".\n",
        "\n",
        "grid_shifted = grid\n",
        "grid_shifted[:, 0] = grid_shifted[:, 0] - centre[0]  # Shift in y-direction.\n",
        "grid_shifted[:, 1] = grid_shifted[:, 1] - centre[1]  # Shift in x-direction.\n",
        "\n",
        "print(\"(y,x) pixel 0 After Shift:\")\n",
        "print(grid_shifted.native[0, 0])  # The coordinate of the first pixel after shifting."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The grid is now centered around $(0.3\", 0.5\")$. We can plot the shifted grid to see this change.\n",
        "\n",
        "*Exercise: Try shifting the grid to a different center, for example $(0.0\", 0.0\")$ or $(2.0\", 3.0\")$. Observe how the center of the grid changes when you adjust the `centre` variable.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_plotter = aplt.Grid2DPlotter(grid=grid_shifted)\n",
        "grid_plotter.set_title(\"Grid Centered Around (0.3, 0.5)\")\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can rotate the grid by an angle $\\phi$ (in degrees). The rotation is counter-clockwise from the positive x-axis.\n",
        "\n",
        "To rotate the grid:\n",
        "\n",
        "1. Calculate the distance `radius` of each coordinate from the origin using $r = \\sqrt{y^2 + x^2}$.\n",
        "2. Determine the angle `theta` counter clockwise from the positive x-axis using $\\theta = \\arctan(y / x)$.\n",
        "3. Adjust `theta` by the rotation angle and convert back to Cartesian coordinates via $y_r = r \\sin(\\theta)$ and $x_r = r \\cos(\\theta)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "angle_degrees = 60.0\n",
        "\n",
        "y = grid_shifted[:, 0]\n",
        "x = grid_shifted[:, 1]\n",
        "\n",
        "radius = np.sqrt(y**2 + x**2)\n",
        "theta = np.arctan2(y, x) - np.radians(angle_degrees)\n",
        "\n",
        "grid_rotated = grid_shifted\n",
        "grid_rotated[:, 0] = radius * np.sin(theta)\n",
        "grid_rotated[:, 1] = radius * np.cos(theta)\n",
        "\n",
        "print(\"(y,x) pixel 0 After Rotation:\")\n",
        "print(grid_rotated.native[0, 0])  # The coordinate of the first pixel after rotation."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The grid has now been rotated 60 degrees counter-clockwise. We can plot it to see the change.\n",
        "\n",
        "*Exercise: Try rotating the grid by a different angle, for example 30 degrees or 90 degrees. Observe how the grid changes when you adjust the `angle_degrees` variable.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_plotter = aplt.Grid2DPlotter(grid=grid_rotated)\n",
        "grid_plotter.set_title(\"Grid Rotated 60 Degrees\")\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we convert the rotated grid to elliptical coordinates using:\n",
        "\n",
        "$\\eta = \\sqrt{(x_r)^2 + (y_r)^2/q^2}$\n",
        "\n",
        "Where `q` is the axis-ratio of the ellipse and $(x_r, y_r)$ are the rotated coordinates. \n",
        "\n",
        "Elliptical coordinates are a system used to describe positions in relation to an ellipse rather than a circle. They \n",
        "are particularly useful in astronomy when dealing with objects like galaxies, which often have elliptical shapes \n",
        "due to their inclination or intrinsic shape.\n",
        "\n",
        "*Exercise: Try converting the grid to elliptical coordinates using a different axis-ratio, for example 0.3 or 0.8. What happens to the grid when you adjust the `axis_ratio` variable?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "axis_ratio = 0.5\n",
        "eta = np.sqrt((grid_rotated[:, 0]) ** 2 + (grid_rotated[:, 1]) ** 2 / axis_ratio**2)\n",
        "\n",
        "print(\"First Ten Elliptical Coordinates:\")\n",
        "print(eta[:10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, the angle $\\phi$ (in degrees) was used to rotate the grid, and the axis-ratio $q$ was used to convert the grid \n",
        "to elliptical coordinates.\n",
        "\n",
        "From now on, we describe ellipticity using \"elliptical components\" $\\epsilon_{1}$ and $\\epsilon_{2}$, calculated \n",
        "from $\\phi$ and $q$:\n",
        "\n",
        "$\\epsilon_{1} = \\frac{1 - q}{1 + q} \\sin(2\\phi)$  \n",
        "$\\epsilon_{2} = \\frac{1 - q}{1 + q} \\cos(2\\phi)$\n",
        "\n",
        "We'll refer to these as `ell_comps` in the code for brevity.\n",
        "\n",
        "*Exercise*: Try computing the elliptical components from the axis-ratio and angle above. What happens to the elliptical\n",
        "components when you adjust the `axis_ratio` and `angle_degrees` variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fac = (1 - axis_ratio) / (1 + axis_ratio)\n",
        "epsilon_y = fac * np.sin(2 * np.radians(angle_degrees))\n",
        "epsilon_x = fac * np.cos(2 * np.radians(angle_degrees))\n",
        "\n",
        "ell_comps = (epsilon_y, epsilon_x)\n",
        "\n",
        "print(\"Elliptical Components:\")\n",
        "print(ell_comps)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Light Profiles__\n",
        "\n",
        "Galaxies are collections of stars, gas, dust, and other astronomical objects that emit light. Astronomers study this \n",
        "light to understand various properties of galaxies.\n",
        "\n",
        "To model the light of a galaxy, we use light profiles, which are mathematical functions that describe how a galaxy's \n",
        "light is distributed across space. By applying these light profiles to 2D grids of $(y, x)$ coordinates, we can \n",
        "create images that represent a galaxy's luminous emission.\n",
        "\n",
        "A commonly used light profile is the `Sersic` profile, which is widely adopted in astronomy for representing galaxy \n",
        "light. The `Sersic` profile is defined by the equation:\n",
        "\n",
        "$I_{\\rm Ser} (\\eta_{\\rm l}) = I \\exp \\left\\{ -k \\left[ \\left( \\frac{\\eta}{R} \\right)^{\\frac{1}{n}} - 1 \\right] \\right\\}$\n",
        "\n",
        "In this equation:\n",
        "\n",
        " - $\\eta$ represents the elliptical coordinates of the profile in arc-seconds (refer to earlier sections for elliptical coordinates).\n",
        " - $I$ is the intensity normalization of the profile, given in electrons per second, which controls the overall brightness of the Sersic profile.\n",
        " - $R$ is the effective radius in arc-seconds, which determines the size of the profile.\n",
        " - $n$ is the Sersic index, which defines how 'steep' the profile is, influencing the concentration of light.\n",
        " - $k$ is a constant that ensures half the light of the profile lies within the radius $R$, where $k = 2n - \\frac{1}{3}$.\n",
        "\n",
        "We can evaluate this function using values for $(\\eta, I, R, n)$ to calculate the intensity of the profile at \n",
        "a particular elliptical coordinate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "elliptical_coordinate = (\n",
        "    0.5  # The elliptical coordinate where we compute the intensity, in arc-seconds.\n",
        ")\n",
        "intensity = 1.0  # Intensity normalization of the profile in electrons per second.\n",
        "effective_radius = 2.0  # Effective radius of the profile in arc-seconds.\n",
        "sersic_index = 1.0  # Sersic index of the profile.\n",
        "k = 2 * sersic_index - (\n",
        "    1.0 / 3.0\n",
        ")  # Calculating the constant k, note that this is an approximation.\n",
        "\n",
        "# Calculate the intensity of the Sersic profile at a specific elliptical coordinate.\n",
        "sersic_value = np.exp(\n",
        "    -k * ((elliptical_coordinate / effective_radius) ** (1.0 / sersic_index) - 1.0)\n",
        ")\n",
        "\n",
        "print(\"Intensity of Sersic Light Profile at Elliptical Coordinate 0.5:\")\n",
        "print(sersic_value)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The calculation above gives the intensity of the Sersic profile at an elliptical coordinate of 0.5.\n",
        "\n",
        "To create a complete image of the Sersic profile, we can evaluate the intensity at every point in our grid of \n",
        "elliptical coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sersic_image = np.exp(-k * ((eta / effective_radius) ** (1.0 / sersic_index) - 1.0))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we plot the resulting image, we can see how the properties of the grid affect its appearance:\n",
        "\n",
        " - The peak intensity is at the position $(0.3\", 0.5\")$, where we shifted the grid.\n",
        " - The image is elongated along a 60\u00b0 counter-clockwise angle, corresponding to the rotation of the grid.\n",
        " - The image has an elliptical shape, consistent with the axis ratio of 0.5.\n",
        "\n",
        "This demonstrates how the geometry of the grid directly influences the appearance of the light profile.\n",
        "\n",
        "*Exercise: Try changing the values of `centre`, `ell_comps`, `effective_radius`, and `sersic_index` above. Observe how these adjustments change the Sersic profile image.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=ag.Array2D(\n",
        "        values=sersic_image, mask=grid.mask\n",
        "    ),  # The `Array2D` object is discussed below.\n",
        ")\n",
        "array_plotter.set_title(\"Sersic Image\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of manually handling these transformations, we can use `LightProfile` objects from the `light_profile` \n",
        "module (`lp`) for faster and more efficient calculations.\n",
        "\n",
        "Below, we define a `Sersic` light profile using the `Sersic` object. We can print the profile to display its parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sersic_light_profile = ag.lp.Sersic(\n",
        "    centre=(0.0, 0.0),\n",
        "    ell_comps=(0.0, 0.1),\n",
        "    intensity=1.0,\n",
        "    effective_radius=2.0,\n",
        "    sersic_index=1.0,\n",
        ")\n",
        "\n",
        "print(sersic_light_profile)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this `Sersic` light profile, we can create an image by passing a grid to its `image_2d_from` method.\n",
        "\n",
        "The calculation will internally handle all the coordinate transformations and intensity evaluations we performed \n",
        "manually earlier, making it much simpler.\n",
        "\n",
        "The `Sersic` profile we created above is different from the one we used to manually compute the image,\n",
        "so the image will look different. However, the process is the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = sersic_light_profile.image_2d_from(grid=grid)\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=image,\n",
        ")\n",
        "array_plotter.set_title(\"Sersic Image via Light Profile\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `image` is returned as an `Array2D` object. \n",
        "\n",
        "Similar to a `Grid2D`, it is accessed via the `native` attribute. which is a 2D NumPy array of shape [total_y_pixels, total_x_pixels]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Intensity of pixel 0:\")\n",
        "print(image.native[0, 0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To visualize the light profile's image, we use a `LightProfilePlotter`.\n",
        "\n",
        "We provide it with the light profile and the grid, which are used to create and plot the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "light_profile_plotter = aplt.LightProfilePlotter(\n",
        "    light_profile=sersic_light_profile, grid=grid\n",
        ")\n",
        "light_profile_plotter.set_title(\"Image via LightProfilePlotter\")\n",
        "light_profile_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `LightProfilePlotter` also has methods to plot the 1D radial profile of the light profile. This profile shows\n",
        "how the intensity of the light changes as a function of distance from the profile's center. This can be a more informative\n",
        "way to visualize the light profile's distribution.\n",
        "\n",
        "The 1D plot below is a `semilogy` plot, meaning that the x-axis (showing the radial distance in arc-seconds) is linear,\n",
        "while the y-axis (showing the intensity) is log10. This is a common way to visualize light profiles, as it highlights\n",
        "the fainter outer regions of the profile. A log x-axis is also a common choice.\n",
        "\n",
        "*Exercise: Try plotting the 1D radial profile of Sersic profiles with different effective radii and Sersic indices. Does the 1D representation show more clearly how the light distribution changes with these parameters?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "light_profile_plotter = aplt.LightProfilePlotter(\n",
        "    light_profile=sersic_light_profile, grid=grid\n",
        ")\n",
        "light_profile_plotter.set_title(\"Sersic 1D Radial Profile\")\n",
        "light_profile_plotter.figures_1d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since galaxy light distributions often cover a wide range of values, they are typically better visualized on a log10 \n",
        "scale. This approach helps highlight details in the faint outskirts of a light profile.\n",
        "\n",
        "The `MatPlot2D` object has a `use_log10` option that applies this transformation automatically. Below, you can see \n",
        "that the image plotted in log10 space reveals more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "light_profile_plotter = aplt.LightProfilePlotter(\n",
        "    light_profile=sersic_light_profile,\n",
        "    grid=grid,\n",
        "    mat_plot_2d=aplt.MatPlot2D(use_log10=True),\n",
        ")\n",
        "light_profile_plotter.set_title(\"Sersic Image\")\n",
        "light_profile_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Galaxies__\n",
        "\n",
        "Now, let's introduce `Galaxy` objects, which are key components in **PyAutoGalaxy**.\n",
        "\n",
        "A light profile represents a single feature of a galaxy, such as its bulge or disk. To model a complete galaxy, \n",
        "we combine multiple `LightProfiles` into a `Galaxy` object. This allows us to create images that include different \n",
        "components of a galaxy.\n",
        "\n",
        "In addition to light profiles, a `Galaxy` has a `redshift`, which indicates how far away it is from Earth. The redshift \n",
        "is essential for performing unit conversions using cosmological calculations, such as converting arc-seconds into \n",
        "kiloparsecs (kpc, a kiloparsec is a distance unit in astronomy, equal to about 3.26 million light-years.)\n",
        "\n",
        "Let's start by creating a galaxy with two `Sersic` light profiles, which notationally we will consider to represent\n",
        "a bulge and disk component of the galaxy, the two most important structures seen in galaxies which drive the\n",
        "Hubble tuning fork classification shown at the beginning of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bulge = ag.lp.Sersic(\n",
        "    centre=(0.0, 0.0),\n",
        "    ell_comps=(0.0, 0.111111),\n",
        "    intensity=1.0,\n",
        "    effective_radius=1.0,\n",
        "    sersic_index=2.5,\n",
        ")\n",
        "\n",
        "disk = ag.lp.Sersic(\n",
        "    centre=(0.0, 0.0),\n",
        "    ell_comps=(0.0, 0.3),\n",
        "    intensity=0.3,\n",
        "    effective_radius=3.0,\n",
        "    sersic_index=1.0,\n",
        ")\n",
        "\n",
        "galaxy = ag.Galaxy(redshift=0.5, bulge=bulge, disk=disk)\n",
        "\n",
        "print(galaxy)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can pass a 2D grid to a light profile to compute its image using the `image_2d_from` method. \n",
        "\n",
        "The same approach works for a `Galaxy` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = galaxy.image_2d_from(grid=grid)\n",
        "\n",
        "print(\"Intensity of `Grid2D` pixel 0:\")\n",
        "print(image.native[0, 0])\n",
        "print(\"Intensity of `Grid2D` pixel 1:\")\n",
        "print(image.native[0, 1])\n",
        "print(\"Intensity of `Grid2D` pixel 2:\")\n",
        "print(image.native[0, 2])\n",
        "print(\"...\")\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=image,\n",
        ")\n",
        "array_plotter.set_title(\"Bulge+Disk Image via Galaxy\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use a `GalaxyPlotter` to plot the galaxy's image, just like how we used `LightProfilePlotter` for a light \n",
        "profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy_plotter = aplt.GalaxyPlotter(galaxy=galaxy, grid=grid)\n",
        "galaxy_plotter.set_title(\"Galaxy Bulge+Disk Image\")\n",
        "galaxy_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bulge dominates the center of the image, and is the majority of luminous emission we see on a linear\n",
        "scale. The disk's emission is present, but it is much fainter and spread over a larger area.\n",
        "\n",
        "We can confirm this using the `subplot_of_light_profiles` method, which plots each individual light profile separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy_plotter.set_title(\"Galaxy Bulge+Disk Subplot\")\n",
        "galaxy_plotter.subplot_of_light_profiles(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because galaxy light distributions often follow a log10 pattern, plotting in log10 space helps reveal details in the \n",
        "outskirts of the light profile, in this case the emission of the disk.\n",
        "\n",
        "This is especially helpful to separate the bulge and disk profiles, which have different intensities and sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy_plotter = aplt.GalaxyPlotter(\n",
        "    galaxy=galaxy, grid=grid, mat_plot_2d=aplt.MatPlot2D(use_log10=True)\n",
        ")\n",
        "galaxy_plotter.set_title(\"Galaxy Bulge+Disk Image\")\n",
        "galaxy_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `figures_1d_decomposed` method allows us to visualize each light profile's contribution in 1D.\n",
        "\n",
        "1D plots show the intensity of the light profile as a function of distance from the profile\u2019s center. The bulge\n",
        "and disk profiles share the same `centre`, meaning that plotting them together shows how they overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy_plotter.set_title(\"Bulge+Disk 1D Decomposed\")\n",
        "galaxy_plotter.figures_1d_decomposed(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can group multiple galaxies at the same redshift into a `Galaxies` object, which is created from a list of \n",
        "individual galaxies.\n",
        "\n",
        "Below, we create an additional galaxy and combine it with the original galaxy into a `Galaxies` object. This could\n",
        "represent two galaxies merging or interacting with each other, which is commonly seen in studies of galaxy evolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "extra_galaxy = ag.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=ag.lp.Sersic(\n",
        "        centre=(0.2, 0.3),\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "galaxies = ag.Galaxies(galaxies=[galaxy, extra_galaxy])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Galaxies` object has similar methods as those for light profiles and individual galaxies.\n",
        "\n",
        "For example, `image_2d_from` sums the images of all the galaxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = galaxies.image_2d_from(grid=grid)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the combined image using a `GalaxiesPlotter`, just like with other plotters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxies_plotter = aplt.GalaxiesPlotter(galaxies=galaxies, grid=grid)\n",
        "galaxies_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A subplot of each individual galaxy image can also be created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxies_plotter.subplot_galaxy_images()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because galaxy light distributions often follow a log10 pattern, plotting in log10 space helps reveal details in the \n",
        "outskirts of the light profile.\n",
        "\n",
        "This is especially helpful when visualizing how multiple galaxies overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxies_plotter = aplt.GalaxiesPlotter(\n",
        "    galaxies=galaxies, grid=grid, mat_plot_2d=aplt.MatPlot2D(use_log10=True)\n",
        ")\n",
        "galaxies_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Unit Conversion__\n",
        "\n",
        "Earlier, we mentioned that a galaxy\u2019s `redshift` allows us to convert between arcseconds and kiloparsecs.\n",
        "\n",
        "A redshift measures how much a galaxy's light is stretched by the Universe's expansion. A higher redshift means the \n",
        "galaxy is further away, and its light has been stretched more. By knowing a galaxy\u2019s redshift, we can convert angular \n",
        "distances (like arcseconds) to physical distances (like kiloparsecs).\n",
        "\n",
        "To perform this conversion, we use a cosmological model that describes the Universe's expansion. Below, we use \n",
        "the `Planck15` cosmology, which is based on observations from the Planck satellite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cosmology = ag.cosmo.Planck15()\n",
        "\n",
        "kpc_per_arcsec = cosmology.kpc_per_arcsec_from(redshift=galaxy.redshift)\n",
        "\n",
        "print(\"Kiloparsecs per Arcsecond:\")\n",
        "print(kpc_per_arcsec)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `kpc_per_arcsec` can be used as a conversion factor between arcseconds and kiloparsecs when plotting images of\n",
        "galaxies.\n",
        "\n",
        "We compute this value and plot the image in converted units of kiloparsecs.\n",
        "\n",
        "This passes the plotting modules `Units` object a `ticks_convert_factor` and manually specified the new units of the\n",
        "plot ticks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "units = aplt.Units(ticks_convert_factor=kpc_per_arcsec, ticks_label=\" kpc\")\n",
        "\n",
        "mat_plot = aplt.MatPlot2D(units=units)\n",
        "\n",
        "galaxy_plotter = aplt.GalaxyPlotter(galaxy=galaxy, grid=grid, mat_plot_2d=mat_plot)\n",
        "galaxy_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "You've learnt the basic quantities used to study galaxy morphology. \n",
        "\n",
        "Lets summarize what we've learnt:\n",
        "\n",
        "- **Grids**: A grid is a set of 2D coordinates that represent the positions where we measure the light of a galaxy. \n",
        "\n",
        "- **Geometry**: How to shift, rotate, and convert grids to elliptical coordinates.\n",
        "\n",
        "- **Light Profiles**: Mathematical functions that describe how a galaxy's light is distributed in space. We've used \n",
        "  the `Sersic` profile to create images of galaxies.\n",
        "\n",
        "- **Galaxies**: Galaxies are collections of light profiles that represent a galaxy's light. We've created galaxies with \n",
        "  multiple light profiles and visualized their images.\n",
        "\n",
        "- **Unit Conversion**: By assuming redshifts for galaxies we can convert their quantities from arcseconds to kiloparsecs.\n",
        "\n",
        "Tutorial 2: Data\n",
        "================\n",
        "\n",
        "In the previous tutorial, we used light profiles to create images of galaxies. However, those images don't accurately\n",
        "represent what we would observe through a telescope.\n",
        "\n",
        "Real telescope images, like those taken with the Charge Coupled Device (CCD) imaging detectors on the Hubble Space\n",
        "Telescope (HST), include several factors that affect what we see:\n",
        "\n",
        "**Telescope Optics:** The optical components of the telescope can blur the light, influencing the image's sharpness.\n",
        "\n",
        "**Exposure Time:** The time the detector collects light, affecting the clarity of the image. Longer exposure times\n",
        "gather more light, improving the signal-to-noise ratio and creating a clearer image.\n",
        "\n",
        "**Background Sky:** Light from a background sky, such as distant stars or zodiacal light, adds noise to the image.\n",
        "\n",
        "In this tutorial, we'll simulate a galaxy image by applying these real-world effects to the light profiles and images\n",
        "we created earlier.\n",
        "\n",
        "Here is an overview of what we'll cover in this tutorial:\n",
        "\n",
        "- **Optics Blurring:** Simulating how the telescope optics blur the galaxy's light, making the image appear blurred.\n",
        "- **Poisson Noise:** Adding Poisson noise to the image, simulating the randomness in the photon-to-electron conversion process on the CCD.\n",
        "- **Background Sky:** Adding a background sky to the image, simulating the light from the sky that adds noise to the image.\n",
        "- **Simulator:** Using the `SimulatorImaging` object to simulate imaging data that includes all these effects.\n",
        "- **Output:** Saving the simulated data to `.fits` files for use in future tutorials, where .fits is the standard image format used by astronomers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from os import path\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "To create our simulated galaxy image, we first need a 2D grid. This grid will represent the coordinate space over \n",
        "which we will simulate the galaxy's light distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid = ag.Grid2D.uniform(\n",
        "    shape_native=(\n",
        "        101,\n",
        "        101,\n",
        "    ),  # The dimensions of the grid, which here is 101 x 101 pixels.\n",
        "    pixel_scales=0.1,  # The conversion factor between pixel units and arc-seconds.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the properties of our galaxy. In this tutorial, we\u2019ll represent the galaxy with a bulge using a \n",
        "Sersic light profile.\n",
        "\n",
        "In the previous tutorial, the units of `intensity` were electrons per second, or $e- pix^-1 s^-1$. To simulate \n",
        "realistic imaging data, the intensity must have these units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy = ag.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=ag.lp.Sersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,  # in units of e- pix^-1 s^-1\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "galaxies = ag.Galaxies(galaxies=[galaxy])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To visualize the galaxy\u2019s image, which we will use as the starting point for the simulations, we use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxies_plotter = aplt.GalaxiesPlotter(galaxies=galaxies, grid=grid)\n",
        "galaxies_plotter.set_title(\"Galaxy Image Before Simulating\")\n",
        "galaxies_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Optics Blurring__\n",
        "\n",
        "All images captured using CCDs (like those on the Hubble Space Telescope) experience some level of blurring \n",
        "due to the optics of the telescope. This blurring occurs because the optical system spreads out the light from each \n",
        "source of light (e.g., a star or a part of a galaxy).\n",
        "\n",
        "The Point Spread Function (PSF) describes how the telescope blurs the image. It can be thought of as a 2D representation \n",
        "of how a single point of light would appear in the image, spread out by the optics. In practice, the PSF is a 2D \n",
        "convolution kernel that we apply to the image to simulate this blurring effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "psf = ag.Kernel2D.from_gaussian(\n",
        "    shape_native=(11, 11),  # The size of the PSF kernel, represented as an 11x11 grid.\n",
        "    sigma=0.1,  # Controls the width of the Gaussian PSF, which determines the level of blurring.\n",
        "    pixel_scales=grid.pixel_scales,  # Maintains consistency with the scale of the image grid.\n",
        "    normalize=True,  # Normalizes the PSF kernel so that its values sum to 1.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the PSF to better understand how it will blur the galaxy's image. The PSF is essentially a small \n",
        "image that represents the spreading out of light from a single point source. This kernel will be used to blur the \n",
        "entire galaxy image when we perform the convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(array=psf)\n",
        "array_plotter.set_title(\"PSF 2D Kernel\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PSF is often more informative when plotted on a log10 scale. This approach allows us to clearly observe values \n",
        "in its tail, which are much smaller than the central peak yet critical for many scientific analyses. The tail \n",
        "values may significantly affect the spread and detail captured in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(array=psf, mat_plot_2d=aplt.MatPlot2D(use_log10=True))\n",
        "array_plotter.set_title(\"PSF 2D Kernel\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll manually perform a 2D convolution of the PSF with the image of the galaxy. This convolution simulates the \n",
        "blurring that occurs when the telescope optics spread out the galaxy's light.\n",
        "\n",
        "1. **Padding the Image**: Before convolution, we add padding (extra space with zero values) around the edges of the \n",
        "   image. This prevents unwanted edge effects when we perform the convolution, ensuring that the image's edges don't \n",
        "   become artificially altered by the process.\n",
        "\n",
        "2. **Convolution**: Using the `Kernel2D` object's `convolve` method, we apply the 2D PSF convolution to the padded \n",
        "   image. This step combines the PSF with the galaxy's light, simulating how the telescope spreads out the light.\n",
        "\n",
        "3. **Trimming the Image**: After convolution, we trim the padded areas back to their original size, obtaining a \n",
        "   convolved (blurred) image that matches the dimensions of the initial galaxy image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = galaxies.image_2d_from(grid=grid)  # The original unblurred image of the galaxy.\n",
        "padded_image = galaxies.padded_image_2d_from(\n",
        "    grid=grid, psf_shape_2d=psf.shape_native  # Adding padding based on the PSF size.\n",
        ")\n",
        "convolved_image = psf.convolved_array_from(\n",
        "    array=padded_image\n",
        ")  # Applying the PSF convolution.\n",
        "blurred_image = convolved_image.trimmed_after_convolution_from(\n",
        "    kernel_shape=psf.shape_native\n",
        ")  # Trimming back to the original size."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot the original and the blurred images. This allows us to clearly see how the PSF \n",
        "convolution affects the appearance of the galaxy, making the image appear softer and less sharp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(array=image)\n",
        "array_plotter.set_title(\"Galaxy Image Before PSF\")\n",
        "array_plotter.figure_2d()\n",
        "\n",
        "array_plotter.set_title(\"Galaxy Image After PSF\")\n",
        "array_plotter = aplt.Array2DPlotter(array=blurred_image)\n",
        "array_plotter.figure_2d()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Poisson Noise__\n",
        "\n",
        "In addition to the blurring caused by telescope optics, we also need to consider Poisson noise when simulating imaging \n",
        "data.\n",
        "\n",
        "When a telescope captures an image of a galaxy, photons from the galaxy are collected by the telescope's mirror and \n",
        "directed onto a CCD (Charge-Coupled Device). The CCD is made up of a silicon lattice (or another material) that \n",
        "converts incoming photons into electrons. These electrons are then gathered into discrete squares, which form the \n",
        "pixels of the final image.\n",
        "\n",
        "The process of converting photons into electrons is inherently random, following a Poisson distribution. This randomness \n",
        "means that the number of electrons in each pixel can vary, even if the same number of photons hits the CCD. Therefore, \n",
        "the electron count per pixel becomes a Poisson random variable. For our simulation, this means that the recorded \n",
        "number of photons in each pixel will differ slightly from the true number due to this randomness.\n",
        "\n",
        "To replicate this effect in our simulation, we can add Poisson noise to the galaxy image using NumPy\u2019s random module, \n",
        "which generates values from a Poisson distribution.\n",
        "\n",
        "It's important to note that the blurring caused by the telescope optics occurs before the photons reach the CCD. \n",
        "Therefore, we need to add the Poisson noise after blurring the galaxy image.\n",
        "\n",
        "We also need to consider the units of our image data. Let\u2019s assume that the galaxy image is measured in units of \n",
        "electrons per second ($e^- s^{-1}$), which is standard for CCD imaging data. To simulate the number of electrons \n",
        "actually detected in each pixel, we multiply the image by the observation\u2019s exposure time. This conversion changes t\n",
        "he units to the total number of electrons collected per pixel over the entire exposure time.\n",
        "\n",
        "Once the image is converted, we add Poisson noise, simulating the randomness in the photon-to-electron conversion \n",
        "process. After adding the noise, we convert the image back to units of electrons per second for analysis, as \n",
        "this is the preferred unit for astronomers when studying their data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exposure_time = 300.0  # Units of seconds\n",
        "blurred_image_counts = (\n",
        "    blurred_image * exposure_time\n",
        ")  # Convert to total electrons detected over the exposure time.\n",
        "blurred_image_with_poisson_noise = (\n",
        "    np.random.poisson(blurred_image_counts, blurred_image_counts.shape) / exposure_time\n",
        ")  # Add Poisson noise and convert back to electrons per second."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is what the blurred image with Poisson noise looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=ag.Array2D(values=blurred_image_with_poisson_noise, mask=grid.mask),\n",
        ")\n",
        "array_plotter.set_title(\"Image With Poisson Noise\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is challenging to see the Poisson noise directly in the image above. To make the noise more \n",
        "visible, we can subtract the blurred image without Poisson noise from the one with noise.\n",
        "\n",
        "This subtraction yields the \"Poisson noise realization\" which highlights the variation in each pixel due to the Poisson \n",
        "distribution of photons hitting the CCD. It represents the noise values that were added to each pixel. We call\n",
        "it the realization because it is one possible outcome of the Poisson process, and the noise will be different each time\n",
        "we simulate the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "poisson_noise_realization = blurred_image_with_poisson_noise - blurred_image\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=ag.Array2D(values=poisson_noise_realization, mask=grid.mask)\n",
        ")\n",
        "array_plotter.set_title(\"Poisson Noise Realization\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Background Sky__\n",
        "\n",
        "The final effect we will consider when simulating imaging data is the background sky.\n",
        "\n",
        "In addition to light from the galaxy, the telescope also picks up light from the sky. This background sky light is \n",
        "primarily due to two sources: zodiacal light, which is light scattered by interplanetary dust in the solar system, \n",
        "and the unresolved emission from distant stars and galaxies.\n",
        "\n",
        "For our simulation, we'll assume that the background sky has a uniform brightness across the image, measured at \n",
        "0.1 electrons per second per pixel. The background sky is added to the image before applying the PSF convolution \n",
        "and adding Poisson noise. This is important because it means that the background contributes additional noise to the \n",
        "image.\n",
        "\n",
        "The background sky introduces noise throughout the entire image, including areas where the galaxy is not present. \n",
        "This is why CCD images often appear noisy, especially in regions far from where the galaxy signal is detected. \n",
        "The sky noise can make it more challenging to observe faint details of the galaxy.\n",
        "\n",
        "To simulate this, we add a constant background sky to the galaxy image and then apply Poisson noise to create the \n",
        "final simulated image as it would appear through a telescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "background_sky_level = 0.1\n",
        "\n",
        "# Add background sky to the blurred galaxy image.\n",
        "blurred_image_with_sky = blurred_image + background_sky_level\n",
        "blurred_image_with_sky_counts = blurred_image_with_sky * exposure_time\n",
        "\n",
        "# Apply Poisson noise to the image with the background sky.\n",
        "blurred_image_with_sky_poisson_noise = (\n",
        "    np.random.poisson(\n",
        "        blurred_image_with_sky_counts, blurred_image_with_sky_counts.shape\n",
        "    )\n",
        "    / exposure_time\n",
        ")\n",
        "\n",
        "# Visualize the image with background sky and Poisson noise.\n",
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=ag.Array2D(values=blurred_image_with_sky_poisson_noise, mask=grid.mask),\n",
        ")\n",
        "array_plotter.set_title(\"Image With Background Sky\")\n",
        "array_plotter.figure_2d()\n",
        "\n",
        "# Create a noise map showing the differences between the blurred image with and without noise.\n",
        "poisson_noise_realization = (\n",
        "    blurred_image_with_sky_poisson_noise - blurred_image_with_sky\n",
        ")\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=ag.Array2D(values=poisson_noise_realization, mask=grid.mask)\n",
        ")\n",
        "array_plotter.set_title(\"Poisson Noise Realization\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Simulator__\n",
        "\n",
        "The `SimulatorImaging` object lets us create simulated imaging data while including the effects of PSF blurring, \n",
        "Poisson noise, and background sky all at once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simulator = ag.SimulatorImaging(\n",
        "    exposure_time=300.0, psf=psf, background_sky_level=0.1, add_poisson_noise=True\n",
        ")\n",
        "\n",
        "dataset = simulator.via_galaxies_from(galaxies=galaxies, grid=grid)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the `data` from the dataset, we can see that it matches the image we simulated earlier. It includes \n",
        "the effects of PSF blurring, Poisson noise, and noise from the background sky. This image is a realistic \n",
        "approximation of what a telescope like the Hubble Space Telescope would capture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.Array2DPlotter(array=dataset.data)\n",
        "dataset_plotter.set_title(\"Simulated Imaging Data\")\n",
        "dataset_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset also includes the `psf` (Point Spread Function) used to blur the galaxy image.\n",
        "\n",
        "For actual telescope data, the PSF is determined during data processing and is provided along with the observations. \n",
        "It's crucial for accurately deconvolving the PSF from the galaxy image, allowing us to recover the true properties \n",
        "of the galaxy. We'll explore this further in the next tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(array=dataset.psf, mat_plot_2d=aplt.MatPlot2D(use_log10=True))\n",
        "array_plotter.set_title(\"Simulated PSF\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset includes a `noise_map`, which represents the Root Mean Square (RMS) standard deviation of the noise \n",
        "estimated for each pixel in the image. Higher noise values mean that the measurements in those pixels are \n",
        "less certain, so those pixels are given less weight when analyzing the data.\n",
        "\n",
        "This `noise_map` is different from the Poisson noise arrays we plotted earlier. The Poisson noise arrays show the \n",
        "actual noise added to the image due to the random nature of photon-to-electron conversion on the CCD, as calculated \n",
        "using the numpy random module. These noise values are theoretical and cannot be directly measured in real telescope data.\n",
        "\n",
        "In contrast, the `noise_map` is our best estimate of the noise present in the image, derived from the data itself. \n",
        "It will be used in the next tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(array=dataset.noise_map)\n",
        "array_plotter.set_title(\"Simulated Noise Map\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `signal-to-noise_map` shows the ratio of the signal in each pixel to the noise level in that pixel. It is \n",
        "calculated by dividing the `data` by the `noise_map`.\n",
        "\n",
        "This ratio helps us understand how much of the observed signal is reliable compared to the noise, allowing us to \n",
        "see where we can trust the detected signal from the galaxy and where the noise is more significant.\n",
        "\n",
        "In general, a signal-to-noise ratio greater than 3 indicates that the signal is likely real and not overwhelmed by \n",
        "noise. For our datasets, the signal-to-noise ratio peaks at ~70, meaning we can trust the signal detected in the\n",
        "image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "array_plotter = aplt.Array2DPlotter(\n",
        "    array=dataset.signal_to_noise_map,\n",
        ")\n",
        "array_plotter.set_title(\"Signal-To-Noise Map\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `ImagingPlotter` object can display all of these components together, making it a powerful tool for visualizing \n",
        "simulated imaging data.\n",
        "\n",
        "It also shows the Data and PSF on a logarithmic (log10) scale, which helps highlight the faint details in these \n",
        "components.\n",
        "\n",
        "The \"Over Sampling\" plots on the bottom of the figures display advanced quantities that you should ignore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "imaging_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "imaging_plotter.set_title(\n",
        "    None\n",
        ")  # Disable input title so subplot uses correct title for each sub-figure.\n",
        "imaging_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "We will now save these simulated data to `.fits` files, the standard format used by astronomers for storing images.\n",
        "Most imaging data from telescopes like the Hubble Space Telescope (HST) are stored in this format.\n",
        "\n",
        "The `dataset_path` specifies where the data will be saved, in this case, in the Google Colab directory \n",
        "`content/BSc_Galaxies_Project/dataset/imaging/simple_example/`, which contains many example images distributed with \n",
        "the `autogalaxy_workspace`.\n",
        "\n",
        "Remember that `content/BSc_Galaxies_Project/` is the root directory of the Google Colab environment, and it therefore\n",
        "is not specified in the `dataset_path` below.\n",
        "\n",
        "The files are named `data.fits`, `noise_map.fits`, and `psf.fits`, and will be used in the next tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"imaging\", \"simple_example\")\n",
        "print(\"Dataset Path: \", dataset_path)\n",
        "\n",
        "dataset.output_to_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    overwrite=True,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, you learned how CCD imaging data of a galaxy is collected using real telescopes like the \n",
        "Hubble Space Telescope, and how to simulate this data using the `SimulatorImaging` object.\n",
        "\n",
        "Let's summarize what we've covered:\n",
        "\n",
        "- **Optics Blurring**: The optics of a telescope blur the light from galaxies, reducing the clarity and sharpness of \n",
        "the images.\n",
        "\n",
        "- **Poisson Noise**: The process of converting photons to electrons on a CCD introduces Poisson noise, which is random \n",
        "variability in the number of electrons collected in each pixel.\n",
        "\n",
        "- **Background Sky**: Light from the sky is captured along with light from the galaxy, adding a layer of noise across \n",
        "the entire image.\n",
        "\n",
        "- **Simulator**: The `SimulatorImaging` object enables us to simulate realistic imaging data by including all of \n",
        "these effects together and contains the `data`, `psf`, and `noise_map` components.\n",
        "\n",
        "- **Output**: We saved the simulated data to `.fits` files, the standard format used by astronomers for storing images.\n",
        "\n",
        "Tutorial 3: Fitting\n",
        "===================\n",
        "\n",
        "In previous tutorials, we used light profiles to create simulated images of galaxies and visualized how these images\n",
        "would appear when captured by a CCD detector on a telescope like the Hubble Space Telescope.\n",
        "\n",
        "However, this simulation process is the reverse of what astronomers typically do when analyzing real data. Usually,\n",
        "astronomers start with an observation\u2014an actual image of a galaxy\u2014and aim to infer detailed information about the\n",
        "galaxy\u2019s properties, such as its shape, structure, formation, and evolutionary history.\n",
        "\n",
        "To achieve this, we must fit the observed image data with a model, identifying the combination of light profiles that\n",
        "best matches the galaxy's appearance in the image. In this tutorial, we'll illustrate this process using the imaging\n",
        "data simulated in the previous tutorial. Our goal is to demonstrate how we can recover the parameters of the light\n",
        "profiles that we used to create the original simulation, as a proof of concept for the fitting procedure.\n",
        "\n",
        "The process of fitting data introduces essential statistical concepts like the `model_model`, `residual_map`, `chi-squared`,\n",
        "`likelihood`, and `noise_map`. These terms are crucial for understanding how fitting works, not only in astronomy but\n",
        "also in any scientific field that involves data modeling. This tutorial will provide a detailed introduction to these\n",
        "concepts and show how they are applied in practice to analyze astronomical data.\n",
        "\n",
        "Here is an overview of what we'll cover in this tutorial:\n",
        "\n",
        "- **Dataset**: Load the imaging dataset that we previously simulated, consisting of the image, noise map, and PSF.\n",
        "- **Mask**: Apply a mask to the data, excluding regions with low signal-to-noise ratios from the analysis.\n",
        "- **Masked Grid**: Create a masked grid, which contains only the coordinates of unmasked pixels, to evaluate the\n",
        "  galaxy's light profile in only unmasked regions.\n",
        "- **Fitting**: Fit the data with a galaxy model, computing key quantities like the model image, residuals,\n",
        "  chi-squared, and log likelihood to assess the quality of the fit.\n",
        "- **Bad Fits**: Demonstrate how even small deviations from the true parameters can significantly impact the fit.\n",
        "- **Model Fitting**: Perform a basic model fit on a simple dataset, adjusting the model parameters to improve the\n",
        "  fit quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from os import path\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We begin by loading the imaging dataset that we will use for fitting in this tutorial. This dataset is identical to the \n",
        "one we simulated in the previous tutorial, representing how a galaxy would appear if captured by a CCD camera.\n",
        "\n",
        "In the previous tutorial, we saved this dataset as .fits files in the `content/BSc_Galaxies_Project/dataset/imaging/simple_example` \n",
        "folder. The `.fits` format is commonly used in astronomy for storing image data along with metadata, making it a\n",
        "standard for CCD imaging.\n",
        "\n",
        "The `dataset_path` below specifies where these files are located: `dataset/imaging/simple_example/`, again\n",
        "relative to the Google Colab root directory `content/BSc_Galaxies_Project/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"imaging\", \"simple_example\")\n",
        "\n",
        "dataset = ag.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Imaging` object contains three key components: `data`, `noise_map`, and `psf`:\n",
        "\n",
        "- `data`: The actual image of the galaxy, which we will analyze.\n",
        "\n",
        "- `noise_map`: The RMS noise-map indicating the uncertainty or noise level in each pixel of the image, reflecting how much the \n",
        "  observed signal in each pixel might fluctuate due to instrumental or background noise.\n",
        "  \n",
        "- `psf`: The Point Spread Function, which describes how a point source of light is spread out in the image by the \n",
        "  telescope's optics. It characterizes the blurring effect introduced by the instrument.\n",
        "\n",
        "Let's print some values from these components and plot a summary of the dataset to refresh our understanding of the \n",
        "imaging data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Value of first pixel in imaging data:\")\n",
        "print(dataset.data.native[0, 0])\n",
        "print(\"Value of first pixel in noise map:\")\n",
        "print(dataset.noise_map.native[0, 0])\n",
        "print(\"Value of first pixel in PSF:\")\n",
        "print(dataset.psf.native[0, 0])\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The signal-to-noise map of the image highlights areas where the signal (light from the galaxy) is detected above the \n",
        "background noise. Values above 3.0 indicate regions where the galaxy's light is detected with a signal-to-noise ratio\n",
        "of at least 3, while values below 3.0 are dominated by noise, where the galaxy's light is not clearly distinguishable.\n",
        "\n",
        "To ensure the fitting process focuses only on meaningful data, we mask out regions with low signal-to-noise \n",
        "ratios, removing areas dominated by noise from the analysis. This allows the fitting process to concentrate on the \n",
        "regions where the galaxy is clearly detected.\n",
        "\n",
        "Here, we create a `Mask2D` to exclude certain regions of the image from the analysis. The mask defines which parts of \n",
        "the image will be used during the fitting process.\n",
        "\n",
        "For our simulated image, a circular 3.0\" mask centered at the center of the image is appropriate, since the simulated \n",
        "galaxy iss positioned at the center."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = ag.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=3.0,  # The circular mask's radius in arc-seconds\n",
        "    centre=(0.0, 0.0),  # center of the image which is also the center of the galaxy\n",
        ")\n",
        "\n",
        "print(mask)  # 1 = True, meaning the pixel is masked. Edge pixels are indeed masked.\n",
        "print(mask[48:53, 48:53])  # Central pixels are `False` and therefore unmasked."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the mask over the galaxy image using an `ImagingPlotter`, which helps us adjust the mask as needed. \n",
        "This is useful to ensure that the mask appropriately covers the galaxy's light and does not exclude important regions.\n",
        "\n",
        "To overlay objects like a mask onto a figure, we use the `Visuals2D` object. This tool allows us to add custom \n",
        "visuals to any plot, providing flexibility in creating tailored visual representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset, visuals_2d=visuals)\n",
        "dataset_plotter.set_title(\"Imaging Data With Mask\")\n",
        "dataset_plotter.figures_2d(data=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we are satisfied with the mask, we apply it to the imaging data using the `apply_mask()` method. This ensures \n",
        "that only the unmasked regions are considered during the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we plot the masked imaging data again, the mask is now automatically included in the plot, even though we did \n",
        "not explicitly pass it using the `Visuals2D` object. The plot also zooms into the unmasked area, showing only the \n",
        "region where we will focus our analysis. This is particularly helpful when working with large images, as it centers \n",
        "the view on the regions where the galaxy's signal is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.set_title(\"Masked Imaging Data\")\n",
        "dataset_plotter.figures_2d(data=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mask is now stored as an additional attribute of the `Imaging` object, meaning it remains attached to the \n",
        "dataset. This makes it readily available when we pass the dataset to a `FitImaging` object for the fitting process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Mask2D:\")\n",
        "print(dataset.mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After applying the mask, the `native` representation of the data changes slighty.\n",
        "\n",
        "The 2D array keeps its original shape, [total_y_pixels, total_x_pixels], but masked pixels (those where the mask is True) are set to 0.0.\n",
        "\n",
        "Let's verify this by checking the `shape_native` of the data and printing a value at the edge which will have been\n",
        "set to 0.0 and a value near the center which will be unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Shape of the masked data:\")\n",
        "print(dataset.data.shape_native)\n",
        "\n",
        "print(\"Example masked pixel in the image's native representation at its edge:\")\n",
        "print(dataset.data.native[0, 0])\n",
        "print(\"Example unmasked pixel in the image's native representation at its center:\")\n",
        "print(dataset.data.native[50, 50])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `mask` object also has a `pixels_in_mask` attribute, which gives the number of unmasked pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(dataset.data.mask.pixels_in_mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Masked Grid__\n",
        "\n",
        "In tutorial 1, we emphasized that the `Grid2D` object is crucial for evaluating a galaxy's light profile. This grid \n",
        "contains (y, x) coordinates for each pixel in the image and is used to map out the positions where the galaxy's \n",
        "light is calculated.\n",
        "\n",
        "From a `Mask2D`, we derive a `masked_grid`, which consists only of the coordinates of unmasked pixels. This ensures \n",
        "that light profile calculations focus exclusively on regions where the galaxy's light is detected, saving computational \n",
        "time and improving efficiency.\n",
        "\n",
        "Below, we plot the masked grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_grid = mask.derive_grid.unmasked\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=masked_grid)\n",
        "grid_plotter.set_title(\"Masked Grid2D\")\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting this masked grid over the galaxy image, we can see that the grid aligns with the unmasked pixels of the \n",
        "image.\n",
        "\n",
        "This alignment **is crucial** for accurate fitting because it ensures that when we evaluate a galaxy's light profile, \n",
        "the calculations occur only at positions where we have real data from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(grid=masked_grid)\n",
        "imaging_plotter = aplt.ImagingPlotter(dataset=dataset, visuals_2d=visuals)\n",
        "imaging_plotter.set_title(\"Image Data With 2D Grid Overlaid\")\n",
        "imaging_plotter.figures_2d(data=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fitting__\n",
        "\n",
        "Now that our data is masked, we are ready to proceed with the fitting process.\n",
        "\n",
        "Fitting the data is done using the `Galaxy` and `Galaxies` objects that we introduced in tutorial 2. We will start by \n",
        "setting up a `Galaxies`` object, using the same galaxy configuration that we previously used to simulate the \n",
        "imaging data. This setup will give us what is known as a 'perfect' fit, as the simulated and fitted models are identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy = ag.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=ag.lp.Sersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,  # in units of e- pix^-1 s^-1\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "galaxies = ag.Galaxies(galaxies=[galaxy])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's plot the image of the galaxies. This should look familiar, as it is the same image we saw in \n",
        "previous tutorials. The difference now is that we use the dataset's `grid`, which corresponds to the `masked_grid` \n",
        "we defined earlier. This means that the galaxy image is only evaluated in the unmasked region, skipping calculations \n",
        "in masked regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxies_plotter = aplt.GalaxiesPlotter(galaxies=galaxies, grid=dataset.grid)\n",
        "galaxies_plotter.set_title(\"Galaxy Image To Be Fitted\")\n",
        "galaxies_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we proceed to fit the image by passing both the `Imaging` and `Galaxies` objects to a `FitImaging` object. \n",
        "This object will compute key quantities that describe the fit\u2019s quality:\n",
        "\n",
        "- `image`: Creates an image of the galaxies using their image_2d_from() method.\n",
        "- `model_data`: Convolves the galaxy image with the data's PSF to account for the effects of telescope optics.\n",
        "- `residual_map`: The difference between the model data and observed data.\n",
        "- `normalized_residual_map`: Residuals divided by noise values, giving units of noise.\n",
        "- `chi_squared_map`: Squares the normalized residuals.\n",
        "- `chi_squared` and `log_likelihood`: Sums the chi-squared values to compute chi_squared, and converts this into a log_likelihood, which measures how well the model fits the data (higher values indicate a better fit).\n",
        "\n",
        "Let's create the fit and inspect each of these attributes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = ag.FitImaging(dataset=dataset, galaxies=galaxies)\n",
        "fit_imaging_plotter = aplt.FitImagingPlotter(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `model_data` represents the galaxy's image after fully accounting for effects like PSF convolution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Central model image pixel:\")\n",
        "print(fit.model_data.native[50, 50])\n",
        "fit_imaging_plotter.figures_2d(model_image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even before computing other fit quantities, we can normally assess if the fit is going to be good by visually comparing\n",
        "the `data` and `model_data` and assessing if they look similar.\n",
        "\n",
        "In this example, the galaxies used to fit the data are the same as the galaxies used to simulate it, so the two\n",
        "look very similar (the only difference is the noise in the image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_imaging_plotter.figures_2d(data=True)\n",
        "fit_imaging_plotter.figures_2d(model_image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `residual_map` is the difference between the observed image and model image, showing where in the image the fit is\n",
        "good (e.g. low residuals) and where it is bad (e.g. high residuals).\n",
        "\n",
        "The expression for the residual map is simply:\n",
        "\n",
        "`residual_map` = (`data` - `model_data`)\n",
        "\n",
        "The residual-map is plotted below, noting that all values are very close to zero because the fit is near perfect.\n",
        "The only non-zero residuals are due to noise in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = dataset.data - fit.model_data\n",
        "print(\"Central residual-map pixel:\")\n",
        "print(residual_map.native[50, 50])\n",
        "\n",
        "print(\"Central residual-map pixel via fit:\")\n",
        "print(fit.residual_map.native[50, 50])\n",
        "\n",
        "fit_imaging_plotter.figures_2d(residual_map=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are these residuals indicative of a good fit to the data? Without considering the noise in the data, it's difficult \n",
        "to ascertain. That is, its hard to ascenrtain if a residual value is large or small because this depends on the\n",
        "amount of noise in that pixel.\n",
        "\n",
        "The `normalized_residual_map` divides the residual-map by the noise-map, giving the residual in units of the noise.\n",
        "Its expression is:\n",
        "\n",
        " `normalized_residual_map` = `residual_map` / `noise_map` = (`data` - `model_data`) / `noise_map`\n",
        "\n",
        "If you're familiar with the concept of standard deviations (sigma) in statistics, the normalized residual map represents \n",
        "how many standard deviations the residual is from zero. For instance, a normalized residual of 2.0 (corresponding \n",
        "to a 95% confidence interval) means that the probability of the model underestimating the data by that amount is only 5%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "normalized_residual_map = residual_map / dataset.noise_map\n",
        "\n",
        "print(\"Central normalized residual-map pixel:\")\n",
        "print(normalized_residual_map.native[50, 50])\n",
        "\n",
        "print(\"Central normalized residual-map pixel via fit:\")\n",
        "print(fit.normalized_residual_map.native[50, 50])\n",
        "\n",
        "fit_imaging_plotter.figures_2d(normalized_residual_map=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the `chi_squared_map`, which is obtained by squaring the `normalized_residual_map` and serves as a \n",
        "measure of goodness of fit.\n",
        "\n",
        "The chi-squared map is calculated as:\n",
        "\n",
        "`chi_squared_map` = (`normalized_residuals`) ** 2.0 = ((`data` - `model_data`)**2.0)/(`variances`)\n",
        "\n",
        "Squaring the normalized residual map ensures all values are positive. For instance, both a normalized residual of -0.2 \n",
        "and 0.2 would square to 0.04, indicating the same quality of fit in terms of `chi_squared`.\n",
        "\n",
        "As seen from the normalized residual map, it is evident that the model provides a good fit to the data, in this\n",
        "case because the chi-squared values are close to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared_map = (normalized_residual_map) ** 2\n",
        "print(\"Central chi-squared pixel:\")\n",
        "print(chi_squared_map.native[50, 50])\n",
        "\n",
        "print(\"Central chi-squared pixel via fit:\")\n",
        "print(fit.chi_squared_map.native[50, 50])\n",
        "\n",
        "fit_imaging_plotter.figures_2d(chi_squared_map=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we consolidate all the information in our `chi_squared_map` into a single measure of goodness-of-fit \n",
        "called `chi_squared`. \n",
        "\n",
        "It is defined as the sum of all values in the `chi_squared_map` and is computed as:\n",
        "\n",
        "`chi_squared` = sum(`chi_squared_map`)\n",
        "\n",
        "This is algebraically written as:\n",
        "\n",
        "$\\chi^2 = \\sum \\left(\\frac{\\text{data} - \\text{model_data}}{\\text{noise_map}}\\right)^2$\n",
        "\n",
        "This summing process highlights why ensuring all values in the chi-squared map are positive is crucial. If we \n",
        "didn't square the values (making them positive), positive and negative residuals would cancel each other out, \n",
        "leading to an inaccurate assessment of the model's fit to the data.\n",
        "\n",
        "The lower the `chi_squared`, the fewer residuals exist between the model's fit and the data, indicating a better \n",
        "overall fit!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared = np.sum(chi_squared_map)\n",
        "print(\"Chi-squared = \", chi_squared)\n",
        "print(\"Chi-squared via fit = \", fit.chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reduced chi-squared is the `chi_squared` value divided by the number of data points (e.g., the number of pixels\n",
        "in the mask). \n",
        "\n",
        "This quantity offers an intuitive measure of the goodness-of-fit, as it normalizes the `chi_squared` value by the\n",
        "number of data points. That is, a reduced chi-squared of 1.0 indicates that the model provides a good fit to the data,\n",
        "because every data point is fitted with a chi-squared value of 1.0.\n",
        "\n",
        "A reduced chi-squared value significantly greater than 1.0 indicates that the model is not a good fit to the data,\n",
        "whereas a value significantly less than 1.0 suggests that the model is overfitting the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reduced_chi_squared = chi_squared / dataset.mask.pixels_in_mask\n",
        "print(\"Reduced Chi-squared = \", reduced_chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another quantity that contributes to our final assessment of the goodness-of-fit is the `noise_normalization`.\n",
        "\n",
        "The `noise_normalization` is computed as the logarithm of the sum of squared noise values in our data: \n",
        "\n",
        "`noise_normalization` = np.sum(np.log(2 * np.pi * `noise_map`**2))\n",
        "\n",
        "This is algebraically written as:\n",
        "\n",
        " $\\sum_{\\rm  j=1}^{J} { \\mathrm{ln}} \\left [2 \\pi (\\text{noise_map})^2 \\right]  \\, .$\n",
        "\n",
        "This quantity is fixed because the noise-map remains constant throughout the fitting process. Despite this, \n",
        "including the `noise_normalization` is considered good practice.\n",
        "\n",
        "Understanding the exact meaning of `noise_normalization` isn't critical for our primary goal of successfully \n",
        "fitting a model to a dataset. Essentially, it provides a measure of how well the noise properties of our data align \n",
        "with a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "noise_normalization = np.sum(np.log(2 * np.pi * dataset.noise_map**2))\n",
        "print(\"Noise Normalization = \", noise_normalization)\n",
        "print(\"Noise Normalization via fit = \", fit.noise_normalization)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the `chi_squared` and `noise_normalization`, we can define a final goodness-of-fit measure known as \n",
        "the `log_likelihood`. \n",
        "\n",
        "This measure is calculated by taking the sum of the `chi_squared` and `noise_normalization`, and then multiplying the \n",
        "result by -0.5:\n",
        "\n",
        "`log_likelihood` = -0.5 * (`chi_squared` + `noise_normalization`)\n",
        "\n",
        "Don't worry about why we multiply by -0.5; it's a standard practice in statistics to ensure the log likelihood is\n",
        "defined correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "print(\"Log Likelihood = \", log_likelihood)\n",
        "print(\"Log Likelihood via fit = \", fit.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous discussion, we noted that a lower `chi_squared` value indicates a better fit of the model to the \n",
        "observed data. \n",
        "\n",
        "When we calculate the log likelihood, we take the `chi_squared` value and multiply it by -0.5. This means that a \n",
        "higher log likelihood corresponds to a better model fit. Our goal when fitting models to data is to maximize the \n",
        "log likelihood.\n",
        "\n",
        "The `reduced_chi_squared`** value provides an intuitive measure of goodness-of-fit. Values close to 1.0 suggest a \n",
        "good fit, while values below or above 1.0 indicate potential underfitting or overfitting of the data, respectively. \n",
        "In contrast, the log likelihood values can be less intuitive. For instance, the log likelihood value printed above \n",
        "is around 5300.\n",
        "\n",
        "However, log likelihoods become more meaningful when we compare them. For example, if we have two models, one with \n",
        "a log likelihood of 5300 and the other with 5310 we can conclude that the first model fits the data better \n",
        "because it has a higher log likelihood by 10.0. \n",
        "\n",
        "In fact, the difference in log likelihood between models can often be associated with a probability indicating how \n",
        "much better one model fits the data compared to another. This can be expressed in terms of standard deviations (sigma). \n",
        "\n",
        "As a rule of thumb:\n",
        "\n",
        "- A difference in log likelihood of **2.5** suggests that one model is preferred at the **2.0 sigma** level.\n",
        "- A difference in log likelihood of **5.0** indicates a preference at the **3.0 sigma** level.\n",
        "- A difference in log likelihood of **10.0** suggests a preference at the **5.0 sigma** level.\n",
        "\n",
        "All these metrics can be visualized together using the `FitImagingPlotter` object, which offers a comprehensive \n",
        "overview of the fit quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = ag.FitImaging(dataset=dataset, galaxies=galaxies)\n",
        "\n",
        "fit_bad_imaging_plotter = aplt.FitImagingPlotter(fit=fit)\n",
        "fit_bad_imaging_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're familiar with model-fitting, you've likely encountered terms like 'residuals', 'chi-squared', \n",
        "and 'log_likelihood' before. \n",
        "\n",
        "These metrics are standard ways in statistics to quantify the quality of a model fit.\n",
        "\n",
        "__Incorrect Fit__\n",
        "\n",
        "In the previous section, we successfully created and fitted a galaxy model to the image data, resulting in an \n",
        "excellent fit. The residual map and chi-squared map showed no significant discrepancies, indicating that the \n",
        "galaxy's light was accurately captured by our model. This optimal solution translates to one of the highest log \n",
        "likelihood values possible, reflecting a good match between the model and the observed data.\n",
        "\n",
        "Now, let's modify our galaxy model to create a fit that is close to the correct solution but slightly off. \n",
        "Specifically, we will slightly offset the center of the galaxy by half a pixel (0.05\") in both the x and y directions. \n",
        "This change will allow us to observe how even small deviations from the true parameters can impact the quality of the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy = ag.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=ag.lp.Sersic(\n",
        "        centre=(0.05, 0.05),  # This is different from the previous center.\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "galaxies = ag.Galaxies(galaxies=[galaxy])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After implementing this slight adjustment, we can now plot the fit. In doing so, we observe that residuals have \n",
        "emerged at the center of the galaxy, which indicates a mismatch between our model and the data. Consequently, \n",
        "this discrepancy results in increased chi-squared values, which in turn affects our log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_bad = ag.FitImaging(dataset=dataset, galaxies=galaxies)\n",
        "\n",
        "fit_bad_imaging_plotter = aplt.FitImagingPlotter(fit=fit_bad)\n",
        "fit_bad_imaging_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can compare the log likelihood of our current model to the log likelihood value we computed previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Previous Likelihood:\")\n",
        "print(fit.log_likelihood)\n",
        "print(\"New Likelihood:\")\n",
        "print(fit_bad.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, we observe that the log likelihood has decreased. \n",
        "\n",
        "This decline confirms that our new model is indeed a worse fit to the data compared to the original model.\n",
        "\n",
        "Now, let\u2019s change our galaxy model once more, this time setting it to a position that is far from the true parameters. \n",
        "We will offset the galaxy's center significantly to see how this extreme deviation affects the fit quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy = ag.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=ag.lp.Sersic(\n",
        "        centre=(\n",
        "            0.65,\n",
        "            0.65,\n",
        "        ),  # This position is significantly different from the previous one.\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "galaxies = ag.Galaxies(galaxies=[galaxy])\n",
        "\n",
        "fit_very_bad = ag.FitImaging(dataset=dataset, galaxies=galaxies)\n",
        "\n",
        "fit_very_bad_imaging_plotter = aplt.FitImagingPlotter(\n",
        "    fit=fit_very_bad,\n",
        ")\n",
        "fit_very_bad_imaging_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is now evident that this model provides a terrible fit to the data. The galaxies do not resemble a plausible \n",
        "representation of our simulated galaxy dataset, which we already anticipated given that we generated the data ourselves.\n",
        "\n",
        "As expected, the log likelihood has dropped dramatically with this poorly fitting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Previous Likelihoods:\")\n",
        "print(fit.log_likelihood)\n",
        "print(fit_bad.log_likelihood)\n",
        "print(\"New Likelihood:\")\n",
        "print(fit_very_bad.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fitting__\n",
        "\n",
        "In the previous sections, we used the true model to fit the data, which resulted in a high log likelihood and minimal \n",
        "residuals. We also demonstrated how even small deviations from the true parameters can significantly degrade the fit \n",
        "quality, reducing the log likelihood.\n",
        "\n",
        "In practice, however, we don't know the \"true\" model. For example, we might have an image of a galaxy observed with \n",
        "the Hubble Space Telescope, but the values for parameters like its `effective_radius`, `sersic_index`, and others are \n",
        "unknown. The process of determining the best-fit model is called model fitting, and it is the main topic of \n",
        "the next tutorial.\n",
        "\n",
        "Let's perform a basic, hands-on model fit to develop some intuition about how we can find the best-fit model. We'll \n",
        "start by loading a simple dataset that was simulated using a `Sersic` profile, where the true parameters of this \n",
        "profile are unknown, and masking it to again exclude regions with low signal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = ag.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = ag.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=3.0,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you'll try to determine the best-fit model for this image, corresponding to the parameters used to simulate the \n",
        "dataset.\n",
        "\n",
        "We'll use the simplest possible approach: try different combinations of light profile parameters and adjust them \n",
        "based on how well each model fits the data. You\u2019ll quickly find that certain parameters produce a much better fit \n",
        "than others. For example, determining the correct values of the `centre` should not take too long.\n",
        "\n",
        "Pay attention to the `log_likelihood` and the `residual_map` as you adjust the parameters. These will guide you in \n",
        "determining if your model is providing a good fit to the data. Aim to increase the log likelihood and reduce the \n",
        "residuals.\n",
        "\n",
        "Keep experimenting with different values for a while, seeing how small you can make the residuals and how high you \n",
        "can push the log likelihood. Eventually, you will reach a point where further improvements become difficult, \n",
        "even after trying many different parameter values. This is a good point to stop and reflect on your first experience \n",
        "with model fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy = ag.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=ag.lp.Sersic(\n",
        "        centre=(1.0, 10),  # These are the parameters\n",
        "        ell_comps=(0.5, 0.9),  # you need to adjust\n",
        "        intensity=1.0,  # to try and improve\n",
        "        effective_radius=1.0,  #  the model's fit\n",
        "        sersic_index=1.0,  # to the data!\n",
        "    ),\n",
        ")\n",
        "\n",
        "galaxies = ag.Galaxies(galaxies=[galaxy])\n",
        "\n",
        "fit = ag.FitImaging(dataset=dataset, galaxies=galaxies)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "print(\"Log Likelihood:\")\n",
        "print(fit.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Manually guessing model parameters repeatedly is a very inefficient and slow way to find the best fit. If the model \n",
        "were more complex\u2014say, if the `galaxy` had additional light profile components beyond just its `bulge` (like a \n",
        "second `Sersic` profile representing a `disk`)\u2014the model would become so intricate that this manual approach \n",
        "would be practically impossible. This is not how model fitting is done in practice.\n",
        "\n",
        "However, this exercise has given you a basic intuition for how model fitting works. The statistical inference tools \n",
        "that are actually used for model fitting will be introduced in the next section. These tools are not entirely different \n",
        "from the approach you just tried. Essentially, they also involve iteratively testing models until those with high log \n",
        "likelihoods are found. The key difference is that a computer can perform this process thousands of times, and it \n",
        "does so in a much more efficient and strategic way.\n",
        "\n",
        "__Linear Light Profile__\n",
        "\n",
        "In the example above, we iteratively adjusted the parameters of a `Sersic` light profile to fit the data. There were\n",
        "7 parameters in total, which is a lot to adjust manually. Any trick which can reduce the number of parameters we need\n",
        "to adjust would make it easier to find high log likelihood solutions.\n",
        "\n",
        "This is possible using **linear light profiles**, which solve for the `intensity` parameter of the light profile via \n",
        "efficient linear algebra. The details of how this works are beyond the scope of this tutorial. The key thing to note \n",
        "is they always compute `intensity` values for each light profile that give the best fit to the data given the other\n",
        "parameter values (e.g. they always maximize the likelihood). \n",
        "\n",
        "Below we show an example using a linear light profile, replacing the code `ag.lp` with `ag_lp_linear`.\n",
        "If you repeat the process above of manually inputing different solutions into the fit in order to find the highest \n",
        "likelihood solutions, you will find higher likelihood values quicker as there are now 6 parameters to adjust instead\n",
        "of 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy = ag.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=ag.lp_linear.Sersic(\n",
        "        centre=(1.0, 10),\n",
        "        ell_comps=(0.5, 0.9),\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=1.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "galaxies = ag.Galaxies(galaxies=[galaxy])\n",
        "\n",
        "fit = ag.FitImaging(dataset=dataset, galaxies=galaxies)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "print(\"Log Likelihood:\")\n",
        "print(fit.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Intensities__\n",
        "\n",
        "The fit contains the solved for intensity values.\n",
        "\n",
        "These are computed using a fit's `linear_light_profile_intensity_dict`, which maps each linear light profile \n",
        "in the model parameterization above to its `intensity`.\n",
        "\n",
        "The code below shows how to use this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(fit.linear_light_profile_intensity_dict)\n",
        "\n",
        "print(\n",
        "    f\"\\n Intensity of galaxy's bulge = {fit.linear_light_profile_intensity_dict[galaxy.bulge]}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, you have learned how to fit a galaxy model to imaging data, a fundamental process in astronomy\n",
        "and statistical inference. \n",
        "\n",
        "Let's summarize what we have covered:\n",
        "\n",
        "- **Dataset**: We loaded the imaging dataset that we previously simulated, consisting of the galaxy image, noise map,\n",
        "  and PSF.\n",
        "  \n",
        "- **Mask**: We applied a circular mask to the data, excluding regions with low signal-to-noise ratios from the analysis.\n",
        "\n",
        "- **Masked Grid**: We created a masked grid, which contains only the coordinates of unmasked pixels, to evaluate the\n",
        "  galaxy's light profile.\n",
        "  \n",
        "- **Fitting**: We fitted the data with a galaxy model, computing key quantities like the model image, residuals,\n",
        "  chi-squared, and log likelihood to assess the quality of the fit.\n",
        "  \n",
        "- **Bad Fits**: We demonstrated how even small deviations from the true parameters can significantly impact the fit\n",
        "  quality, leading to decreased log likelihood values.\n",
        "  \n",
        "- **Model Fitting**: We performed a basic model fit on a simple dataset, adjusting the model parameters to improve the\n",
        "  fit quality.\n",
        "\n",
        "\n",
        "Tutorial 4: Non-linear Search\n",
        "=============================\n",
        "\n",
        "The starting point for most scientific analysis conducted by an Astronomer is that they have observations of a galaxy\n",
        "using a telescope like the Hubble Space Telescope, and seek to learn about the galaxy and the Universe from these\n",
        "observations. With **PyAutoGalaxy**, we seek to learn about the galaxy's structure and morphology, asking questions like\n",
        "how big is the galaxy, is it disky or bulgy, and how is its light distributed?\n",
        "\n",
        "To answer these questions, we must therefore fit the dataset with a model of the galaxy, where the model defines the\n",
        "light profile that make up the galaxy we fit. Our goal is the find the combination of light profile parameters that\n",
        "best-fit the data, such that the model represents the galaxy, and therefore the Universe, as well as possible.\n",
        "\n",
        "This process is called model-fitting, or \"modeling\" for short. This is close to what we just did in the tutorials \n",
        "above, where fit a simulated dataset as if the parameters used to simulate the data were unknown and you manually \n",
        "guessed the values of the parameters that best fit the data. You iteratively improved the model-fit by guessing new \n",
        "parameters, over and over, finding solutions which produced higher `log_likelihood` values.\n",
        "\n",
        "However, this approach was not optimal: it was manual and slow, we had no certainty that we had found the best\n",
        "(e.g. maximum likelihood) solution and for more complex models, with more parameters and light profiles, it would\n",
        "have been unfeasible.\n",
        "\n",
        "In this tutorial, we perform modeling as a scientist would actually do it, and introduce the statistical inference\n",
        "technique that will ultimately allow us to fit complex models made of many light profiles to real galaxy data,\n",
        "and begin learning about real galaxies in the Universe.\n",
        "\n",
        "This section introduces a number of key statistical concepts that are fundamental to understanding how\n",
        "model-fitting works, both for **PyAutoGalaxy** and in general.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "This tutorial is split into the following sections:\n",
        "\n",
        "- **Parameter Space**: Introduce the concept of a \"parameter space\" and how it relates to model-fitting.\n",
        "- **Non-Linear Search**: Introduce the concept of a \"non-linear search\" and how it fits models to data.\n",
        "- **Data**: Load and plot the galaxy dataset we'll fit.\n",
        "- **Model**: Introduce the galaxy model we'll fit to the data.\n",
        "- **Priors**: Introduce priors and how they are used to define the parameter space and guide the non-linear search.\n",
        "- **Analysis**: Introduce the `Analysis` class, which contains the `log_likelihood_function` used to fit the model to the data.\n",
        "- **Nested Sampling**: Perform a model-fit using the nested sampling search.\n",
        "- **Result**: The result of the model-fit, including the maximum likelihood model.\n",
        "- **Samples**: The samples of the non-linear search, used to compute parameter estimates and uncertainties.\n",
        "- **Customizing Searches**: How to customize the settings of the non-linear search.\n",
        "- **Wrap Up**: A summary of the concepts introduced in this tutorial.\n",
        "\n",
        "__Parameter Space__\n",
        "\n",
        "In mathematics, a function is defined by its parameters, which map inputs to outputs. For example, consider the simple function:\n",
        "\n",
        "f(x) = x^2\n",
        "\n",
        "Here, \\(x\\) is the input parameter, and f(x) returns the output x^2. This relationship defines\n",
        "the \"parameter space\" of the function, which in this case forms a parabola.\n",
        "\n",
        "Functions can also have multiple parameters, such as:\n",
        "\n",
        "f(x, y, z) = x + y^2 - z^3\n",
        "\n",
        "This defines a parameter space in three dimensions, representing the relationships between \\(x\\), \\(y\\), \\(z\\),\n",
        "and the output f(x, y, z).\n",
        "\n",
        "This concept of parameter space is closely related to how we approach model-fitting. For instance, in the previous\n",
        "tutorials, we created instances of a `Galaxy` object with\n",
        "parameters like \\(`centre_0`, `centre_1`, `ell_comps_0`, `ell_comps_1`, `intensity`, `effective_radius`, `sersic_index`\\).\n",
        "These parameters were used to fit data and compute a log likelihood.\n",
        "\n",
        "We can think of this process as analogous to the function: \n",
        "\n",
        "f(`centre_0`, `centre_1`, `ell_comps_0`, `ell_comps_1`, `intensity`, `effective_radius`, `sersic_index`),\n",
        "\n",
        "where the output of this function, f, is the log likelihood. This function, which maps parameter values to a log likelihood, is known\n",
        "as the \"likelihood function\" in statistical inference. To be explicit, we\u2019ll refer to it as the `log_likelihood_function`\n",
        "since it returns the natural log of the likelihood function.\n",
        "\n",
        "By framing the likelihood this way, we can think of our model as having its own parameter space\u2014a multidimensional\n",
        "surface defined by all possible values of the\n",
        "parameters (`centre_0`, `centre_1`, `ell_comps_0`, `ell_comps_1`, `intensity`, `effective_radius`, `sersic_index`).\n",
        "This surface, known as the \"likelihood surface,\" represents how the log likelihood changes across different parameter\n",
        "values. During model-fitting, our goal is to find the peak of this surface, where the fit to the data is optimal.\n",
        "\n",
        "This parameter space is \"non-linear,\" meaning the relationship between the model parameters and the log likelihood is\n",
        "not a simple linear one. Because of this non-linearity, we cannot predict the log likelihood from a given set of model\n",
        "parameters without actually performing a fit to the data, as we did above by manually inputting different parameter\n",
        "values and assessing the log likelihood.\n",
        "\n",
        "__Non-Linear Search__\n",
        "\n",
        "Now that we understand our problem in terms of a non-linear parameter space with a likelihood surface, we can\n",
        "introduce the method used to fit the model to the data \u2014- the \"non-linear search\".\n",
        "\n",
        "Previously, our approach involved manually guessing models until finding one with a good fit and high log likelihood.\n",
        "Surprisingly, this random guessing forms the basis of how model-fitting using a non-linear search actually works!\n",
        "\n",
        "A non-linear search involves systematically guessing many models while tracking their log likelihoods. As the\n",
        "algorithm progresses, it tends to favor models with parameter combinations that have previously yielded higher\n",
        "log likelihoods. This iterative refinement helps to efficiently explore the vast parameter space.\n",
        "\n",
        "There are two key differences between guessing random models and using a non-linear search:\n",
        "\n",
        "- **Computational Efficiency**: The non-linear search can evaluate the log likelihood of a model parameter\n",
        "  combination in milliseconds and therefore many thousands of models in minutes. This computational speed enables\n",
        "  it to thoroughly sample potential solutions, which would be impractical for a human.\n",
        "\n",
        "- **Effective Sampling**: The search algorithm maintains a robust memory of previously guessed models and their log\n",
        "  likelihoods. This allows it to sample potential solutions more thoroughly and converge on the highest\n",
        "  likelihood solutions more efficiently, which is again impractical for a human.\n",
        "\n",
        "Think of the non-linear search as systematically exploring parameter space to pinpoint regions with the highest log\n",
        "likelihood values. Its primary goal is to identify and converge on the parameter values that best describe the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from os import path\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__PyAutoFit__\n",
        "\n",
        "Modeling uses the probabilistic programming language\n",
        "[PyAutoFit](https://github.com/rhayes777/PyAutoFit), an open-source project that allows complex model\n",
        "fitting techniques to be straightforwardly integrated into scientific modeling software. \n",
        "\n",
        "We import this library separately from **PyAutoGalaxy**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "Let's first load the `Imaging` dataset, which we will use to fit a model with a non-linear search.\n",
        "\n",
        "The galaxy in this image was generated using a `Sersic` light profile, which we'll also use in our model fitting \n",
        "in this tutorial. This means the model we are going to fit is identical to the one used to simulate the data, \n",
        "allowing us to assess the fitting process under controlled conditions.\n",
        "\n",
        "The dataset, as well as all subsequent datasets used in future tutorials, is stored in \n",
        "the `/dataset/imaging` folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = ag.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The fit requires a mask, which we define as a 3.0\" circle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = ag.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "To compose a model, we set up a `Galaxy` using a `af.Model`. Instead of manually specifying every parameter \n",
        "for the galaxy's light profiles (as we did before), we will now define the galaxy using only the class of each \n",
        "profile. Using a `Model` object tells **PyAutoGalaxy** that the parameters of the profiles should be fitted for \n",
        "during the non-linear search.\n",
        "\n",
        "In this case, we'll model the galaxy with an elliptical Sersic light profile, which represents \n",
        "its bulge component (the same profile used to simulate the galaxy).\n",
        "\n",
        "We again use the linear light profile variant via `ag.lp_linear`, which makes the modeling process more accurate\n",
        "and efficient by not having to fit for the `intensity` parameter of the light profile as a dimension in parameter space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "galaxy_model = af.Model(ag.Galaxy, redshift=0.5, bulge=ag.lp_linear.Sersic)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now input the model component into a `Collection` object, which groups all the model components used to fit the data.\n",
        "\n",
        "As with profiles, we give galaxies descriptive names. Since this model has only one galaxy, we'll name it as `galaxy` \n",
        "below.\n",
        "\n",
        "It may seem odd that we define two `Collections`, with the `Collection` in the outer loop only having a `galaxies`\n",
        "attribute. For certain tasks, we may have multiple galaxies in a model, and the code below would allow us to therefore\n",
        "add multiple galaxies to the model. For now, however, we only have one galaxy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(galaxies=af.Collection(galaxy=galaxy_model))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Priors__\n",
        "\n",
        "When we examine the `model.info`, we notice that each parameter (like `centre`, `effective_radius`, \n",
        "and `sersic_index` in our `Sersic` model) is associated with priors, such as `UniformPrior`. Priors define the \n",
        "range of permissible values that each parameter can assume during the model fitting process, for example a uniform\n",
        "prior means that a parameter is equally likely to be any value within the given range, but cannot be outside of it.\n",
        "\n",
        "The priors displayed above use default values which are broad, and contain the breadth of plausible solutions one \n",
        "should expect when fitting light profiles to a real galaxy.\n",
        "\n",
        "For instance, consider the `centre` parameter of our `Sersic` light profile. In theory, it could take on any value from \n",
        "negative to positive infinity. However, imaging datasets are reduced such that the galaxy centre is close \n",
        "to (0.0\", 0.0\"). Therefore, a `UniformPrior` with `lower_limit=-0.3` and `upper_limit=0.3` is a good description of where the\n",
        "galaxy `centre` is. \n",
        "\n",
        "For certain tasks, the galaxy have have a different centre in the dataset, therefore the prior would be \n",
        "updated to reflect this.\n",
        "\n",
        "Priors serve two primary purposes:\n",
        "\n",
        "**Defining Valid Parameter Space:** Priors specify the range of parameter values that constitute valid solutions. \n",
        "This ensures that our model explores only those solutions that are consistent with our observed data and physical \n",
        "constraints.\n",
        "\n",
        "**Incorporating Prior Knowledge:** Priors also encapsulate our prior beliefs or expectations about the model \n",
        "parameters. For instance, if we have previously fitted a similar model to another dataset and obtained certain \n",
        "parameter values, we can incorporate this knowledge into our priors for a new dataset. This approach guides the \n",
        "model fitting process towards parameter values that are more probable based on our prior understanding.\n",
        "\n",
        "Below, we manually specify the priors on all parameter in our `Sersic` model. \n",
        "\n",
        "The custom priors below are close to the default priors, with the main purpose of the code below to show you how to \n",
        "customize priors yourself. \n",
        "\n",
        "However, by making priors more specific to the dataset we are fitting, for example spanning a narrower range of\n",
        "values around the anticipated value of a parameter, we can speed up the non-linear search. Therefore, below we:\n",
        "\n",
        "- Reduce the priors on the `centre` values from between -0.3\" and 0.3\" to -0.1\" and 0.1\".\n",
        "- Reduce the prior on the `ell_comps` values from between -1.0 and 1.0 to -0.5 and 0.5, which still allow for very elliptical solutions which most galaxies will be covered by."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.galaxies.galaxy.bulge.centre.centre_0 = af.UniformPrior(lower_limit=-0.1, upper_limit=0.1)\n",
        "model.galaxies.galaxy.bulge.centre.centre_1 = af.UniformPrior(lower_limit=-0.1, upper_limit=0.1)\n",
        "model.galaxies.galaxy.bulge.ell_comps.ell_comps_0 = af.UniformPrior(\n",
        "    lower_limit=-0.5, upper_limit=0.5\n",
        ")\n",
        "model.galaxies.galaxy.bulge.ell_comps.ell_comps_1 = af.UniformPrior(\n",
        "    lower_limit=-0.5, upper_limit=0.5\n",
        ")\n",
        "model.galaxies.galaxy.bulge.effective_radius = af.UniformPrior(\n",
        "    lower_limit=0.0, upper_limit=10.0\n",
        ")\n",
        "model.galaxies.galaxy.bulge.sersic_index = af.UniformPrior(\n",
        "    lower_limit=0.8, upper_limit=8.0\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By reprinting the `model.info`, we can see that the priors have been updated to the values we specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The `AnalysisImaging` object defines how an instance of a model, consisting of a set of parameters values for the \n",
        "light profiles, is fitted to the `Imaging` dataset.\n",
        "\n",
        "The fit is performed using the analysis class's `log_likelihood_function`, which in model-fitting is a commonly used \n",
        "term to describe a function that given a model and data, fits the model to the data to return a value of log \n",
        "likelihood. \n",
        "\n",
        "In sections 1, 2 and 3 you essentially performed this likelihood function yourself by hand, when you \n",
        "entered different models to a `FitImaging` object and used its `log_likelihood` property to quantify how well it \n",
        "fitted the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = ag.AnalysisImaging(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Searches__\n",
        "\n",
        "we now perform a non-linear search to fit the model of the galaxy to the data.\n",
        "\n",
        "**Nested Sampling** is an advanced method for model-fitting that excels in fitting complex models with intricate \n",
        "parameter spaces. \n",
        "\n",
        "Here\u2019s a simplified overview of its process:\n",
        "\n",
        "1. Start with a set of \"live points\" in parameter space, each initialized with random parameter values drawn from their respective priors.\n",
        "\n",
        "2. Compute the log likelihood for each live point.\n",
        "\n",
        "3. Draw a new point based on the likelihoods of the current live points, such that a point inside of a higher region of parameter space is likely to be drawn.\n",
        "\n",
        "4. If the new point has a higher likelihood than any existing live point, it becomes a live point, and the lowest likelihood live point is discarded.\n",
        "\n",
        "This iterative process continues, gradually focusing the live points around higher likelihood regions of parameter \n",
        "space until they converge on the highest likelihood solution.\n",
        "\n",
        "Nested Sampling effectively maps out parameter space, providing accurate estimates of parameters and their uncertainties.\n",
        "It features a built-in stopping criterion, meaning the fit will stop running automatically when it has sampled all\n",
        "of parameter space and identified the model with the highest likelihood. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    name=\"example_0\",\n",
        "    n_live=50,\n",
        "    iterations_per_update=1000,\n",
        "    force_x1_cpu=True # This ensures the Google Colab code runs correctly\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin the model-fit via the non-linear search, we pass it our model and analysis and begin the fit.\n",
        "\n",
        "Model fitting typically takes between 2 and 10 minutes to run, which means the Jupyter notebook cell will be running\n",
        "for this duration of time. Run the cell below to begin the non-linear search, and then read the cell afterwards\n",
        "whilst it is running in order to understand how you can inspect the results of the fit during and after it has\n",
        "completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "Owing to the relatively long run times of model fitting, the results are output and stored in a folder which you can \n",
        "manually inspect whilst the Jupyter notebook cell is running. \n",
        "\n",
        "In Google Colab, you can access this folder by clicking the folder icon on the left of the screen and navigating to \n",
        "the `BSc_Galaxies_Project/output` folder. This will contain a folder named `example`, corresponding to the input\n",
        "`name=example` we specified when we created the non-linear search above. \n",
        "\n",
        "The screenshot below shows how your Google Colab should appear when you click the folder icon, with red squares\n",
        "highlighting the output folder and other folders which are important and described in the next paragraph:\n",
        "\n",
        "![ColabFolderOutput](https://github.com/Jammy2211/BSc_Galaxies_Project/blob/master/Colab_Example_Folder_Output.png?raw=true)\n",
        "\n",
        "Inside this folder is a folder which is a collection of characters, which is a unique identifier which ensures if you \n",
        "rerun the Jupyter notebook cell it loads the results from the previous run, thus saving time by not rerunning the\n",
        "non-linear search.\n",
        "\n",
        "Inside the unique identifier folder are a number of files you should inspect:\n",
        "\n",
        " - `model.info`: Summarizes the model, its parameters and their priors.\n",
        " \n",
        " - `model.results`: Summarizes the highest likelihood model inferred so far including errors.\n",
        " \n",
        " - `images`: Visualization of the highest likelihood model-fit to the dataset as a file called `subplot_fit.png`.\n",
        "\n",
        "The files `model.results` and those contained in `images` are only generated after the non-linear search has completed\n",
        "`iteration_per_update` number of iterations, which for the input value above of 1000 will take approximately 1 minute.\n",
        "\n",
        "The Jupyter notebook cell will display when it outputs these results, so you should monitor the cell and look\n",
        "for these files once it has performed an update.\n",
        "\n",
        "__Result__\n",
        "\n",
        "Upon completion the non-linear search returns a `Result` object, which contains information about the model-fit.\n",
        "\n",
        "The `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing the result contains we use now is the `FitImaging` object that corresponds to the set of model\n",
        "parameters that gave the maximum log likelihood solution. \n",
        "\n",
        "We plot this object to inspect how good our fit was, showing an excellent fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Permanently Saving Results__\n",
        "\n",
        "A Google Colab is a temporary server that is erased after 90 minutes, meaning your output folder will be\n",
        "erased and you will lose the results of any model-fitting you performed.\n",
        "\n",
        "There are two ways to permanently save the results of a model-fit, and you can use either or both of them.\n",
        "\n",
        "**Save to Google Drive**: If you have a Google Drive, you can save the results to it. This is handled by the\n",
        "code below, which every time you run it will save the results in the `output` folder of your Google Drive\n",
        "in a folder named `BSc_Galaxies_Project`.\n",
        "\n",
        "If you navigate to this folder in your Google Drive you will find the results of all model-fits you have\n",
        "performed. As new runs are performed and results are saved, they will be added to this folder ever time you\n",
        "run the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!cp -r /content/BSc_Galaxies_Project/output /content/drive/MyDrive/BSc_Galaxies_Project"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Download to Hard-disc**: You can download the `output` folder to your PC or laptop by running the code below. \n",
        "\n",
        "You have to .zip the output folder first in order to download it, and you may therefore need to also unzip it\n",
        "locally once downloaded. \n",
        "\n",
        "As you perform more runs, this .zip file will include them, ensuring all results of model fitting are retained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r output.zip /content/BSc_Galaxies_Project/output\n",
        "files.download(\"output.zip\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "This tutorial has laid the foundation with several fundamental concepts in model fitting and statistical inference:\n",
        "\n",
        "1. **Parameter Space**: This refers to the range of possible values that each parameter in a model can take. It \n",
        "defines the dimensions over which the likelihood of different parameter values is evaluated.\n",
        "\n",
        "2. **Likelihood Function**: This function receives as input a model with specific parameter values and returns a\n",
        "log likelihood value that quantifies how well the model describes the data.\n",
        "\n",
        "3. **Likelihood Surface**: This surface represents how the likelihood of the model varies across the parameter space. \n",
        "It helps in identifying the best-fit parameters that maximize the likelihood of the model given the data.\n",
        "\n",
        "4. **Non-linear Search**: This is an optimization technique used to explore the parameter space and find the \n",
        "combination of parameter values that best describe the data. It iteratively adjusts the parameters to maximize the \n",
        "likelihood. Many different search algorithms exist, each with their own strengths and weaknesses, and this tutorial\n",
        "used nested sampling search called Nautilus.\n",
        "\n",
        "5. **Priors**: Priors are probabilities assigned to different values of parameters before considering the data. \n",
        "They encapsulate our prior knowledge or assumptions about the parameter values. Priors can constrain the parameter \n",
        "space, making the search more efficient and realistic.\n",
        "\n",
        "6. **Model Fitting**: The process of adjusting model parameters to minimize the difference between model predictions \n",
        "and observed data, quantified by the likelihood function.\n",
        "\n",
        "Understanding these concepts is crucial as they form the backbone of model fitting and parameter estimation in \n",
        "scientific research and data analysis. \n",
        "\n",
        "You are now ready to begin the research project you have been assigned, where model fitting will be used to fit real\n",
        "images of galaxies from the James Webb Space Telescope and research a particular aspect of galaxy morphology."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}